# üö¢ Titanic Survival Prediction ‚Äì Dual Model Approach with Feature Engineering

This project tackles the Titanic survival prediction challenge by strategically splitting the dataset based on the presence or absence of cabin information, and by applying robust feature engineering and model selection techniques to enhance predictive performance.

---

## üß© 1. Dataset Splitting Based on Cabin Information

Approximately **77%** of the records in the dataset lacked values in the `cabin` column. Instead of discarding this information, the dataset was divided into two subsets:

- `df_no_deck`: Passengers **without** cabin information.
- `df_with_deck`: Passengers **with** cabin information.

This allowed for the creation of **two specialized models**, each tailored to the quality of available features for the respective group.

---

## üß™ 2. Model Design and Feature Engineering

- For passengers **without deck information**, the model `model_without_deck` was based on **LightGBM** and enhanced with categorical variables like `sex`, `embarked`, and `agegroup`.
  
- For passengers **with deck information**, the model `model_with_deck` was built using **Logistic Regression**, incorporating a new engineered feature called `deck`, derived from the `cabin` column. This transformation turned missing data into meaningful structure, revealing privilege levels among passengers.

---

## ‚öôÔ∏è 3. Model Evaluation and Selection

Each subset was evaluated separately using **Stratified K-Fold Cross-Validation** and performance metrics such as **ROC-AUC** to identify the best discriminative models.

### üìä Key Results:

**With Deck (`model_with_deck`)**:
- Logistic Regression: AUC = **0.9364**
- XGBoost: AUC = 0.9303
- LightGBM: AUC = 0.9015

**Without Deck (`model_without_deck`)**:
- LightGBM outperformed all other models and was selected as the best baseline.

üß† Additionally, an attempt was made to optimize LightGBM using **GridSearchCV**, but the optimized model underperformed compared to its base version (AUC dropped from **0.6827** to **0.6633**), likely due to overfitting or suboptimal search space.

---

## üß† 4. Model Integration and Prediction System

A custom router class called **`TitanicModelRouter`** was implemented. It:
- Accepts any DataFrame as input.
- Detects whether each row has a value in the `cabin` column.
- Applies the corresponding preprocessing pipeline.
- Uses the appropriate trained model to generate predictions.

This hybrid approach ensures a **robust and flexible prediction pipeline**, tailored to the **real structure and limitations of the available data**.

---

## üì§ 5. Submission and Kaggle Score

The final predictions were exported into a file called `submissions.csv` and submitted to the [Titanic Kaggle competition](https://www.kaggle.com/c/titanic/overview).  
‚úÖ The model achieved a score of **0.69856**, confirming the robustness of the dual-model strategy and the value of thoughtful feature engineering and model routing.

---

## ‚úÖ Conclusion

This project highlights the impact of:
- Smart feature engineering.
- Model specialization based on data availability.
- Careful evaluation using class-imbalance-aware metrics (like ROC-AUC).

The resulting architecture adapts dynamically to data quality and leverages the best model for each case, significantly improving predictive performance.


